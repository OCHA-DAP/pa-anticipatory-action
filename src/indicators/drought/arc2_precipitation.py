import logging
import os
from datetime import date, datetime
from itertools import groupby
from operator import itemgetter
from pathlib import Path
from typing import Tuple, Union

import geopandas as gpd
import numpy as np
import pandas as pd
import requests
import rioxarray
import xarray as xr

from src.utils_general.raster_manipulation import compute_raster_statistics

logger = logging.getLogger(__name__)

DATA_DIR = Path(os.environ["AA_DATA_DIR"])
PUBLIC_DATA_DIR = "public"
RAW_DATA_DIR = "raw"
PROCESSED_DATA_DIR = "processed"
ARC2_DIR = "arc2"

# TODO: Should add in parameters that control whether
# or not you want to save the intermediate / raw data

# TODO: Parameters should also control whether or not a function,
# like computing dry spells, uses pre downloaded data or whether
# it should download the data in one go

# TODO: Error handling for various things,
# like if data is missing or certain files don't exist
# I've been using logger (Seth) but will turn to raising
# exceptions as I've been learning more about better error
# handling

# TODO: Better logging

# TODO: Write tests


class ARC2:
    """Summary of class

    TODO: fill

    Attributes:
        country_iso3: ISO3 string.
        date_min: Minimum date to load data from, either string
            in ISO 8601 format, e.g. '2021-03-20' or `datetime.date` object.
        date_max: Maximum date to load data from, either string
            in ISO 8601 format, e.g. '2021-04-20' or `datetime.date` object.
        range_x: Tuple of strings specifying longitude range
            for download, e.g. ('32E', '36E')
        range_y: Tuple of strings specifying latitude range
            for download, e.g. ('20S', '5S')
    """

    def __init__(
        self,
        country_iso3: str,
        date_min: Union[str, date],
        date_max: Union[str, date],
        range_x: Tuple[str, str],
        range_y: Tuple[str, str],
    ):
        self.country_iso3 = country_iso3

        if not isinstance(date_min, date):
            date_min = date.fromisoformat(date_min)
        self.date_min = date_min

        if not isinstance(date_max, date):
            date_max = date.fromisoformat(date_max)
        self.date_max = date_max

        self.range_x = range_x
        self.range_y = range_y

    def _download(
        self, date_min_dl: date, date_max_dl: date, master: bool = False
    ):

        """
        Download data from IRI servers for a specific date range
        and longitude/latitude bounds. If `master`, the file
        is fully loaded as the master file. Otherwise, it is
        temporarily loaded and merged into the master file before
        removal.

        :param date_min_dl: `datetime.date` specifying first date
            to download from.
        :param date_max_dl: `datetime.date` specifying last date
            to download from.
        :param master: Boolean specifying whether download is master
            file. If `True`, existing master is removed and full
            file is saved. If `False`, download is temporarily saved
            and then merged to master and deleted.
        """

        url = (
            f"https://iridl.ldeo.columbia.edu/SOURCES/.NOAA/.NCEP/.CPC/.FEWS/"
            f".Africa/.DAILY/.ARC2/.daily/.est_prcp/T/"
            f"%28{date_min_dl:%-d%%20%b%%20%Y}%29"
            f"%28{date_max_dl:%-d%%20%b%%20%Y}%29RANGEEDGES/"
            f"X/%28{self.range_x[0]}%29%28{self.range_x[1]}%29RANGEEDGES/"
            f"Y/%28{self.range_y[0]}%29%28{self.range_y[1]}%29RANGEEDGES/"
            f"data.nc"
        )

        raw_filepath = self._get_raw_filepath(master=master)

        if os.path.exists(raw_filepath) and master:
            logger.info(
                "Removing existing master raw ARC2 file before re-downloading."
            )
            os.remove(raw_filepath)

        cookies = {
            "__dlauth_id": os.getenv("IRI_AUTH"),
        }

        logger.info(
            f"Downloading ARC2 data from {self.date_min:%d %b %Y} to "
            f"{self.date_max:%d %b %Y} "
            f"covering longitudes {self.range_x[0]} to {self.range_x[1]} "
            f"and latitudes {self.range_y[0]} to {self.range_y[1]}."
        )

        # TODO: explore specific errors generated by request
        try:
            response = requests.get(url, cookies=cookies, verify=False)
        except requests.exceptions.RequestException as e:
            raise SystemExit(e)

        # create folders if necessary and write file
        Path(raw_filepath.parent).mkdir(parents=True, exist_ok=True)

        with open(raw_filepath, "wb") as fd:
            for chunk in response.iter_content(chunk_size=128):
                fd.write(chunk)

        # merge to master and delete temporary files
        if not master:
            master_filepath = self._get_raw_filepath(master=True)
            if not os.path.isfile(master_filepath):
                logger.error(
                    (
                        "Master raster data file not set. This needs "
                        "to be done once by running "
                        "`download_data(master=True)` "
                        "for a unique set of coordinates."
                    )
                )

            logger.info(
                f"Merging ARC2 data from {self.date_min:%d %b %Y} "
                f"to {self.date_max:%d %b %Y} "
                f"into master file: {os.path.basename(master_filepath)}."
            )

            with xr.open_dataarray(master_filepath) as ds:
                master = ds.load()
            with xr.open_dataarray(raw_filepath) as ds:
                raw = ds.load()

            master_merge = xr.concat([master, raw], dim="T")

            os.remove(raw_filepath)
            os.remove(master_filepath)

            if "_FillValue" in master_merge.encoding and np.isnan(
                master_merge.encoding["_FillValue"]
            ):
                master_merge.encoding["_FillValue"] = -999

            master_merge.to_netcdf(master_filepath)

        return

    def _get_directory(self, dir: Union[Path, str]) -> Path:
        """
        Return ARC2 directory, either in public, processed
        or raw for a specific ISO3.

        :param dir: Path or string of folder name in public for passing.
        """
        directory = (
            DATA_DIR / PUBLIC_DATA_DIR / dir / self.country_iso3 / ARC2_DIR
        )
        return directory

    def _get_raw_filepath(self, master: bool) -> Path:
        """
        Return filepath to raw ARC2 data for specific x
        and y bounds. All data stored within a single
        master file. If `master`, the filepath is not
        specified for a specific date range and designated
        with 'master.nc' at the end. Otherwise, end of
        filepath designated with `self.date_min` and
        `self.date_max`.

        :param master: Whether or not the filepath is
            for the master file.
        :param temp: Whether or not the filepath is
            for a temp file used for saving.
        """
        directory = self._get_directory(RAW_DATA_DIR)

        if master:
            end = "master"
        else:
            end = f"{self.date_min:%d_%b_%Y}_{self.date_max:%d_%b_%Y}"

        filename = (
            f"arc2_daily_precip_{self.country_iso3}_"
            f"{self.range_x[0]}_{self.range_x[1]}_"
            f"{self.range_y[0]}_{self.range_y[1]}_{end}.nc"
        )
        return directory / Path(filename)

    def _get_processed_filepath(self, agg_method: str = "centroid") -> Path:
        """
        Return filepath to processed ARC2 data aggregated to ADM2 level
        using arithmetic mean. All data stored within a single master file.
        """
        directory = self._get_directory(PROCESSED_DATA_DIR)
        filename = (
            f"arc2_{agg_method}_long_{self.country_iso3}_"
            f"{self.range_x[0]}_{self.range_x[1]}_"
            f"{self.range_y[0]}_{self.range_y[1]}_"
            f"master.csv"
        )
        return directory / Path(filename)

    def _get_monitoring_filepath(self) -> Path:
        """
        Return filepath to ARC2 dry spell monitoring file.
        """
        directory = self._get_directory(PROCESSED_DATA_DIR) / "monitoring"
        filename = f"{self.date_max}_results.txt"
        return directory / Path(filename)

    def _write_to_monitoring_file(self, dry_spells: Union[None, int] = None):
        monitoring_file = self._get_monitoring_filepath(
            self.country_iso3, self.date_max
        )
        """
        Write a simple output of the number
        of dry spells observed in the last 14 days.
        """
        result = ""
        with open(monitoring_file, "w") as f:
            if dry_spells is not None:
                result += "No dry spells identified in the last 14 days."
                f.write(result)
            else:
                f.write(
                    f"Dry spells identified in \
                    {len(dry_spells)} admin regions:\n{dry_spells}"
                )
        f.close()
        return

    def load_raw_data(self, raw_filepath: Union[Path, None] = None):
        """
        Convenience function to load raw raster data, squeeze
        it and write its CRS. The function always accesses
        the master file.

        :param raw_filepath: Path to raw file to load. If `None`,
            loads master file.
        """
        if raw_filepath is None:
            raw_filepath = self._get_raw_filepath(master=True)

        # load raw master data
        try:
            with rioxarray.open_rasterio(raw_filepath, masked=True) as ds:
                da = ds.load()
                da = da.squeeze().rio.write_crs("EPSG:4326")

        except Exception:
            logger.error(
                "Raw raster file %s does not exist.",
                os.path.basename(raw_filepath),
            )

        return da

    def download_data(self, master: bool = False):
        """
        Download ARC2 data for all dates between `self.date_min`
        and `self.date_max`. If `master`, then all data
        is downloaded directly from the servers and set as the
        master file. If not `master`, then data
        is only downloaded for dates not already
        available in the master file, and then merged
        into the master file.

        :param master: Boolean on whether to set download as
            master if `True`, or only download missing data
            and merge to master if `False`.
        """
        if master:
            self._download(self.date_min, self.date_max, master)
        else:
            # load master data and find all dates covered
            # compare to min/max and then download missing
            mr = self.load_raw_data()
            loaded_dates = mr.indexes[
                "T"
            ].to_datetimeindex() - pd.to_timedelta("12:00:00")
            full_dates = pd.date_range(self.date_min, self.date_max)
            needed_dates = full_dates.difference(loaded_dates)
            if len(needed_dates) > 0:
                date_ranges = self._group_date_ranges(needed_dates)
                for dates in date_ranges:
                    self._download(
                        date_min_dl=dates[0],
                        date_max_dl=dates[1],
                        master=False,
                    )

                self._sort_raw_data()
            else:
                logger.info("No additional data needs downloading.")

    def _group_date_ranges(self, dates) -> list:
        """
        Group range of dates into list of consecutive dates to pass to
        `self._download()` for calling to the API. For each group, only
        the min and max dates are returned rather than the full list
        of dates in the group.

        :param dates: Date range generated by `pd.date_range()`
        """
        date_ranges = []

        for _, g in groupby(
            enumerate(dates),
            key=lambda x: x[0] - (x[1] - datetime(1970, 1, 1)).days,
        ):
            group = map(itemgetter(1), g)
            group = list(map(pd.Timestamp, group))
            if len(group) == 1:
                group.append(group[0])
            date_ranges.append((group[0], group[-1]))

        return date_ranges

    def _sort_raw_data(self):
        """
        Sort master file by time coordinates to ensure
        correct ordering.
        """
        master_filepath = self._get_raw_filepath(master=True)
        with xr.open_dataarray(master_filepath) as ds:
            master = ds.load()

        master.sortby("T").to_netcdf(master_filepath)

    def process_data(
        self,
        polygon_path: Union[Path, str] = None,
        bound_col: str = None,
        all_touched: bool = False,
        reprocess: bool = False,
    ):
        """
        Get mean aggregation by admin boundary for the downloaded arc2 data.
        Outputs a csv with daily aggregated statistics. If data already
        processed between `self.date_min` and `self.date_max`, returns
        pre-processed data, otherwise processes additional data and joins to
        processed master file.

        :param polygon_path: Path to polygon file for clipping and downsampling
            raster data.
        :param bound_col: Column in polygon file to aggregate raster to.
        :param all_touched: Boolean, to use centroids or all touching rasters.
        :param reprocess: Boolean, if `True` reprocesses all raster data.
            Otherwise, only processes dates that have not already been
            processed.
        """

        if all_touched:
            agg_method = "touching"
        else:
            agg_method = "centroid"

        processed_filepath = self._get_processed_filepath(agg_method)

        da = self.load_raw_data()

        # load polygon data
        try:
            gdf = gpd.read_file(polygon_path)
        except Exception:
            logger.error(
                "Clip file %s does not exist.", os.path.basename(polygon_path)
            )

        # only process data for dates that have not already been processed
        if os.path.exists(processed_filepath) and not reprocess:
            exist_stats = pd.read_csv(processed_filepath, parse_dates=["T"])
            lookup = da.indexes["T"]
            lookup = ~lookup.isin(exist_stats["T"])
            if np.sum(lookup) == 0:
                logger.info(
                    "No additional dates to process for %s.",
                    processed_filepath,
                )
                return exist_stats

            else:
                da = da.loc[lookup, :, :]
        else:
            Path(processed_filepath.parent).mkdir(parents=True, exist_ok=True)

        df_zonal_stats = compute_raster_statistics(
            gdf=gdf,
            bound_col=bound_col,
            raster_array=da,
            all_touched=all_touched,
            stats_list=["mean"],
        )

        # join up existing data if necessary
        if "exist_stats" in locals():
            df_zonal_stats = df_zonal_stats.append(
                exist_stats, ignore_index=True
            )
            df_zonal_stats.sort_values(by=["T"])

        # infill missing data with interpolation
        data_col = "mean_" + bound_col
        if "infilled" not in df_zonal_stats.columns:
            df_zonal_stats["infilled"] = True

        df_zonal_stats["infilled"] = np.where(
            df_zonal_stats[data_col].isna(), False, df_zonal_stats["infilled"]
        )

        df_zonal_stats[data_col] = df_zonal_stats.groupby(bound_col)[
            data_col
        ].transform(lambda x: x.interpolate())

        df_zonal_stats.to_csv(processed_filepath, index=False)
        return df_zonal_stats

    def identify_dry_spells(
        self,
        rolling_window: int = 14,
        rainfall_mm: int = 2,
        agg_method: str = "centroid",
    ):
        """
        Read the processed data and check if any dry spells occurred
        in that time period. Processed data should cover >= 14 days of
        precipitation. We're assuming that this data is from during
        the rainy season.

        :param rolling_window: Number of days for rolling sum of precipitation.
        :param rainfall_mm: Maximum precipitation during window to
            classify as dry spell.
        :param agg_method: One of 'centroid' or 'touching'.
        """

        processed_file = self._get_processed_filepath(agg_method)

        # TODO:
        # 1. Read in processed data
        df = pd.read_csv(processed_file)

        # 2. Check that it covers the min days needed to define a dry spell

        # 3. Calculate the rolling sum
        adm_col = df.columns[2]
        precip_col = df.columns[1]
        grouped = df.groupby(adm_col)[precip_col].rolling(rolling_window).sum()

        # 4. Identify dry spells based on rolling sum (rainfall_mm)
        print(grouped)

        # 5. Notify if any admin areas are in a dry spell
        return
