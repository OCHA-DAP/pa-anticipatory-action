{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noteboook to test the Fuzzywuzzy library https://github.com/seatgeek/fuzzywuzzy for geolocation of the survey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.environ['AA_DATA_DIR']\n",
    "df_srv = pd.read_csv(os.path.join(data_dir, 'exploration', 'bangladesh', 'CDP_Survey', 'secondround_locations.csv'))\n",
    "df_srv['C04_mouza_name']=df_srv['C04_mouza_name'].replace('\\*','',regex=True).astype(str)\n",
    "df_shp = gpd.read_file(os.path.join(data_dir, 'exploration', 'bangladesh', 'ADM_Shp', 'selected_distict_mauza.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce duplicates we need to use the full combination df admin names\n",
    "In the shapefile even the combination of Mauza, Union, Upazilla and District is still not always unique.\n",
    "This is in many cases due to issues in the inputs shapefile. It doesn't impact too much the statistical analysis as they would be in the same area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_mozas=df_shp[['MAUZNAME','UNINAME','THANAME','DISTNAME']].value_counts()\n",
    "duplicate_mozas=duplicate_mozas.where(duplicate_mozas>1).dropna()\n",
    "print(duplicate_mozas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new string that is the combination of Mauza, Union, Upazilla and District names, both for the survey data and for the shapefile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_shp=['MAUZNAME','UNINAME','THANAME','DISTNAME']\n",
    "cols_srv=['C04_mouza_name','C04_union_name','C04_upazila_name','C04_district_name']\n",
    "\n",
    "df_shp[cols_shp]=df_shp[cols_shp].astype(str)\n",
    "df_srv[cols_srv]=df_srv[cols_srv].astype(str)\n",
    "\n",
    "df_shp['shp_full_name']= df_shp[cols_shp].agg(','.join, axis=1)\n",
    "df_srv['srv_full_name']= df_srv[cols_srv].agg(','.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_best_fuzzy_match(srv_name,df_shp,threshold=70):\n",
    "# returns the best match for srv_name among all the features in the shapefile\n",
    "# the features with the highest score above the threshold is returned\n",
    "# if there are multiple matches it selects the feature with the largest area\n",
    "# if there are no matching features returns None\n",
    "# the fuzzy matching is done usind scores from https://github.com/seatgeek/fuzzywuzzy\n",
    "    \n",
    "    # calculate all fuzzy scores relative to srv_name\n",
    "    scores = df_shp['shp_full_name'].apply(lambda x: fuzz.ratio(x,srv_name))\n",
    "    # create a dataframe to perform some operations\n",
    "    data = pd.DataFrame(\n",
    "            {'shp_id': df_shp['OBJECTID'],\n",
    "            'shp_area': df_shp['Shape_Area'],\n",
    "            'shp_full_name': df_shp['shp_full_name'],\n",
    "            'fuzzy_score': scores}\n",
    "            )\n",
    "    max_score=data['fuzzy_score'].max()\n",
    "    if max_score<threshold:\n",
    "        return {\n",
    "                'srv_full_name':srv_name,\n",
    "                'n_matches':0,\n",
    "                'matching_score':max_score\n",
    "                }\n",
    "    # get only results with maximum scores\n",
    "    data=data.loc[data['fuzzy_score']==max_score]\n",
    "    n_matches=len(data)\n",
    "    # if there are multiple choices we get the largest one\n",
    "    data=data.loc[data['shp_area'].idxmax()]\n",
    "    return {\n",
    "            'srv_full_name':srv_name,\n",
    "            'shp_id':data['shp_id'],\n",
    "            'shp_full_name':data['shp_full_name'],\n",
    "            'n_matches':n_matches,\n",
    "            'matching_score':max_score\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function defined above we can now match the survey names (srv_name) with the corresponding name in teh shapefile and the OBJECTID of the shapefile feature\n",
    "We are only matching the set of survey names to speed up computing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_srv_full_name=df_srv['srv_full_name'].unique()\n",
    "matching_df=pd.DataFrame()\n",
    "# TODO put in function\n",
    "for srv_full_name in set_srv_full_name:\n",
    "    matching_df=matching_df.append(get_best_fuzzy_match(srv_full_name,df_shp),ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally do a left join on the survey data to get the OBJECTID of the shapefile associated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_srv=pd.merge(left=df_srv,right=matching_df,left_on='srv_full_name',right_on='srv_full_name',how='left')\n",
    "\n",
    "# check total #of rows\n",
    "print(f'total rows:{len(matched_srv)}')\n",
    "\n",
    "# check multiple matches\n",
    "print('multiple matches: {}'.format(len(matched_srv[matched_srv['n_matches']>1])))\n",
    "\n",
    "# check no matches\n",
    "print('No matches: {}'.format(len(matched_srv[matched_srv['n_matches']==0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally merge back the shapefile and see # of people per union and district to see if it matches the inputs\n",
    "This is done using the RECONSTRUCTED geolocation and it's useful to cross check the quality of the geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_srv_shp=pd.merge(left=matched_srv,right=df_shp,left_on='shp_id',right_on='OBJECTID',how='left',suffixes=['','_shp'])\n",
    "matched_srv_shp=matched_srv_shp.drop(['shp_full_name_shp','geometry'],axis=1)\n",
    "\n",
    "\n",
    "print('Number of interviewees by district')\n",
    "print(matched_srv_shp.groupby('DISTNAME').size())\n",
    "\n",
    "print('Number of interviewees by union')\n",
    "print(matched_srv_shp.groupby('UNINAME').size())\n",
    "\n",
    "matched_srv.to_csv(os.path.join(os.path.join(data_dir, 'exploration', 'bangladesh', 'CDP_Survey', 'secondround_locations_matched.csv')))\n",
    "#writer = pd.ExcelWriter(f'{dir_path}/mauza_data/household_locations_impactevaluation_matched.xlsx', engine='xlsxwriter')\n",
    "#matched_srv_shp.to_excel(writer,sheet_name='matched_data')\n",
    "#matched_srv_shp.groupby('DISTNAME').size().to_excel(writer,sheet_name='QA_HH_District')\n",
    "#matched_srv_shp.groupby('UNINAME').size().to_excel(writer,sheet_name='QA_HH_Union')\n",
    "#writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
